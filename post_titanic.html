<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Titanic Tutorial</title>
    <link rel="icon" type="image/x-icon" href="images/ecology.png">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-GLhlTQ8iRABdZLl6O3oVMWSktQOp6b7In1Zl3/Jr59b6EGGoI1aFkw7cmDA6j6gD" crossorigin="anonymous">
</head>
<body>
    <section>
        <nav class="navbar  navbar-expand-lg bg-body-tertiar " style="background-color: #e3f2fd;">
      <div class="container-fluid">
        <img src="images/ecology.png" alt="pro" width="40px">
        <a class="navbar-brand px-2" href="index.html">Preetham Paul Chinta</a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
          <ul class="navbar-nav">
            <li class="nav-item">
              <a class="nav-link active" aria-current="page" href="index.html">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="index.html#edu">Resume</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="index.html#projects">Projects</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
        </section>


        <section>
            <div class="container-fluid">
                <div class="row mx-5 ">
                    
        
                        <div class="col-12  py-2">
                            <h1 class=" mt-3">Kaggle Titanic Tutorial</h1>
                            <h5 class="fs-3 mt-3">Objective</h5>

                            <p class="fs-6">
                                In this ML Tutorial, we will analyze Titanic Dataset.
                                We will preprocess the data and run Random Forest model to predict if a person will survive 
                                the infamous titanic accident or not.
                                We need to showcase what group of people are most likely to survive.
                            </p>
            
                        </div>
                       
                    
                </div>
                <!-- section -->
                <div class="row mx-5">
                    <div class="col-12 py-1">
                        <h5 class="fs-3">Dataset</h5>

                        <p class="fs-6">
                            The <a href="https://www.kaggle.com/c/titanic/data" target="_blank">dataset</a> represents the details of each passenger who encountered the deadly man-made disaster in 1912, it has 9 features namely:
                            <ul>
                                <li>Passenger Id</li>
                                <li>Passenger Class (Pclass)</li>
                                <li>Passenger Name (Name)</li>
                                <li>Gender (sex)</li>
                                <li>SibSp</li>
                                <li>Fare</li>
                                <li>Parch</li>
                                <li>Ticket no.</li>
                                <li>Survived</li>
                            </ul>
                            With the help of these features we will extract insights.
                      
                        </p>
        
                    </div>
                </div>
                <!-- section -->
                <div class="row mx-5">
                    <div class="col-12  py-1">
                        <h5 class="fs-3">Implementation (Tutorial)</h5>
                        <p>
                            In the tutorial, we created dataframes of given dataset using pandas library.
                            From this dataset we select 4 features (Pclass,Sex,SibSp,Parch)
                            , the tutorial specified these features and Survived attribute to train the data on Random Forest Classifier 
                            imported from sklearn. 
                            After that we run the trained model on test dataset. We then calculate the accuracy score of given submission values with the predicted output of test set.
                            According to the <a href="https://www.kaggle.com/alexisbcook/titanic-tutorial" target="_blank ">tutorial</a> the accuracy score of 0.9712%.
                        </p>
                    </div>
                </div>

                 <!-- section -->
                 <div class="row mx-5">
                  <div class="col-12  py-1">
                      <h5 class="fs-3">Contribution</h5>
                      <p>
                          From the given dataset, there are more features where we can extract more insights which can lead to even more accurate prediction.
                          I performed 2 experiments with these other features which were not included in the Random forest classifier in the tutorial.
                          Here is my <a href="https://www.kaggle.com/code/paulieeee/chintapaul?scriptVersionId=121728094" target="_blank">notebook</a>.
                          
                      </p>

                      <h5>Preprocessing of Fare and Age features</h5>
                      <p> 
                        <div class="bg-light p-5">
                          <code class="text-muted">
                                                        # all missing values <br>
                            print("Nan's in train age column = "+str(train_data.Age.isna().sum() ))<br>
                            print("Nan's in test age column = "+str(test_data.Age.isna().sum() ))<br>
                            print("Nan's in train Fare column = "+str(train_data.Fare.isna().sum() ))<br>
                            print("Nan's in test Fare column = "+str(test_data.Fare.isna().sum() ))<br>
                            <br>
                            # changin data type<br>
                            print("Age data in train set is of type "+ str(train_data.Age.dtype))<br>
                            print("Age data in test set is of type "+ str(test_data.Age.dtype))<br>
                            <br>
                            # preprocessing Fare feature to add to our random forest model<br>
                            <br>
                            # imputing missing Fare with mean, <br>
                            # converting continous to categorical for train <br>
                            <br>
                            test_data["Fare"].fillna(test_data["Fare"].mean(),inplace=True) #no missing values in train set<br>
                            train_data['Fare'] = pd.cut(train_data['Fare'], bins=[0,50,150,513],labels=['Less Expensive','More Expensive','Most Expensive'])<br>
                            test_data['Fare'] = pd.cut(test_data['Fare'], bins=[0,50,150,513],labels=['Less Expensive','More Expensive','Most Expensive'])<br>
                            </code>
                        </div>
                      </p>

                      <h5>Experiment 1</h5>
                      <p> We take Fare feature from the dataset which is in type float64. From the price of the ticket we can assume a number of hypothesis.
                        <ol><li>Position in the ship: It might be the case where people who paid different price for ticket might have different sections in the ship which might have impacted the chances of survival.</li>
                          <li>Safety of the passenger:Safety measures may vary depending on the price of ticket in case of accident. we wil analyse to see if its true or not.</li>
                      </ol>
                      From this point of view, I generated a graph to plot Fare to survival.Since the ticket price is a continous distribution of type float64, I performed 
                      preprocessing methods(imputation by mode) to clean and handle missing data to. 
                      Converting the continous data to categorical data(less expensive,more expensive,most expensive) so it would run in the Random Forest model.<br>
                      <img src="images/fare.png" alt="fare.png" style="display: block;margin-left: auto;margin-right: auto;width: 50%;">
                      These are the insights that can be established from the graph(cleaned and processed data):<br>
                      Passengers who have purchased expensive ticket had 69% survival rate where others who have purchased least expensive had 32% survival rate.
                      Thus confirming the two hypothesis stated above "True".
                      <div class="bg-light p-5">
                        <code class="text-muted">
                          # calculating Fare vs Survive<br>
                          <br>
                          print("Percentage of People of Fare group Died:")<br>
                          print(train_data[train_data['Survived']==0].Fare.value_counts()/train_data.Fare.value_counts())<br>
                          <br>
                          print("Percentage of People of Fare group Survived:")<br>
                          print(train_data[train_data['Survived']==1].Fare.value_counts()/train_data.Fare.value_counts())<br>
                          <br>
                          # we can say that people who brought Most Expensive ticket had more chance of survival<br>
                          <br>
                          #output<br>
                          >><br>
                          Percentage of People of Fare group Died:<br>
                          Less Expensive    0.675978<br>
                          More Expensive    0.320611<br>
                          Most Expensive    0.310345<br>
                          Name: Fare, dtype: float64<br>
                          Percentage of People of Fare group Survived:<br>
                          Less Expensive    0.324022<br>
                          More Expensive    0.679389<br>
                          Most Expensive    0.689655<br>
                          Name: Fare, dtype: float64<br>
                        </code>
                      </div>
                      </p>
                      

                      <h5>Experiment 2</h5>
                      <p> Then if we access the Age feature from the dataset which is in type float64.
                        From the Age of the passenger we can assume a 2 hypothesis.
                        <ol>
                          <li>Most unlikely to survive: We can have a guess of which group of people might have a hard time trying to escape, we will analyse, based on age if elderly people are most unlikely to survive or not. </li>
                          <li>Law of human nature:In any accident we are bound to evacuate younger people(children) as chivalrous act, we will try to analyse if its true.</li>
                      </ol>
                      From this point of view, I generated a graph to plot Age to survival.Since the Age is of type float64, I performed 
                      preprocessing methods(data conversion,imputation by mode) to clean and handle missing data to. 
                      Converting the data to categorical data(Teen,Adult,Elderly) so it would run in the Random Forest model.<br>
                      <img src="images/age.png" alt="fare.png" style="display: block;margin-left: auto;margin-right: auto;width: 50%;">
                     
                      These are the insights that can be established from the graph(cleaned and processed data):<br>
                      Passengers who are the oldest had very less chance of survival (30%).
                      Teen(children) had high survival rate(45%) compared to other groups.
                      Thus confirming the two hypothesis stated above "True".
                      <div class="bg-light p-5">
                        <code class="text-muted">
                          # calculating Age vs Survive<br>
                          <br>
                          print("Percentage of People of Age group Died:")<br>
                          print(train_data[train_data['Survived']==0].Age.value_counts()/train_data.Age.value_counts())<br>
                          <br>
                          print("Percentage of People of Age group Survived:")<br>
                          print(train_data[train_data['Survived']==1].Age.value_counts()/train_data.Age.value_counts())<br>
                          <br>
                          # we can say that Elder people could not handle the titanic accident the most.<br>
                          #output<br>
                          >><br>
                          Percentage of People of Age group Died:<br>
                          Adult    0.635174<br>
                          Teen     0.541401<br>
                          Elder    0.692308<br>
                          Name: Age, dtype: float64<br>
                          Percentage of People of Age group Survived:<br>
                          Adult    0.364826<br>
                          Teen     0.458599<br>
                          Elder    0.307692<br>
                          Name: Age, dtype: float64<br>
                        </code>
                      </div>
  
                      </p>

                      <h5>Improvement</h5>
                      
                      
                      <p> We now train our model using Random forest classifier to check the impact of adding 2 additional features(Age,Fare).
                        After training and testing on the data. we generate submission.csv and calculate the accuracy score of the given data.
                      </p>
                      <div class="bg-light p-5">
                        <code class="text-secondary">
                      from sklearn.ensemble import RandomForestClassifier<br>
                      <br>
                        y = train_data["Survived"]<br>
                        <br>
                        features = ["Pclass", "Sex", "SibSp", "Parch","Fare","Age"] #added preprocessed Fare and Age columns<br>
                        X = pd.get_dummies(train_data[features])<br>
                        X_test = pd.get_dummies(test_data[features])<br>
                        <br>
                        model = RandomForestClassifier(n_estimators=100, max_depth=5,random_state=1)<br>
                        model.fit(X, y)<br>
                        predictions = model.predict(X_test)<br>
                        <br>
                        output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})<br>
                        output.to_csv('submission.csv', index=False)<br>
                        print("Your submission was successfully saved!")<br>
                        after=accuracy_score(df_sub['Survived'],output['Survived'])<br>
                        print("Accuracy score for given submission file is = "+str(after))<br>
                        <br>
                        <span class="text-muted">

                          #output<br>
                          >><br>
                          Your submission was successfully saved!<br>
                          Accuracy score for given submission file is = 0.9832535885167464<br>
                        </span>
                      </code>
                    </div>
                    <p>Original score: <strong> 0.971</strong> <br>
                     Improved score: <strong>0.983</strong>
                 </p>


                  </div>
              </div>
              <!-- section -->
              
        </div>
        </section>





</body>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js" integrity="sha384-w76AqPfDkMBDXo30jS1Sgez6pr3x5MlQ1ZAGC+nuZB+EYdgRZgiwxhTBTkF7CXvN" crossorigin="anonymous"></script>

</html>